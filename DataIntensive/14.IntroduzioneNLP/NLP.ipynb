{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Laboratorio: Natural Language Processing\n",
    "\n",
    "**Programmazione di Applicazioni Data Intensive**  \n",
    "Laurea in Ingegneria e Scienze Informatiche  \n",
    "DISI - Università di Bologna, Cesena\n",
    "\n",
    "Proff. Gianluca Moro, Roberto Pasolini  \n",
    "`nome.cognome@unibo.it`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "- Con _Natural Language Processing_ (NLP) ci si riferisce all'insieme di tecniche per il processamento di **testo in linguaggio naturale** (inglese, italiano, ...)\n",
    "- Obiettivo del NLP è estrarre **informazioni di alto livello** dal testo o convertirlo in una **forma strutturata** (es. vettori e matrici) trattabile da altri algoritmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLTK\n",
    "\n",
    "- _NLTK_ (_Natural Language Toolkit_) è una delle principali librerie Python per il trattamento di testi in linguaggio naturale\n",
    "- Fornisce diversi algoritmi, spesso usati come componenti per pre-processare documenti di testo nell'ambito di analisi di dati\n",
    "- NLTK è già inclusa in Colab e Anaconda, in altri casi può essere installata con pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importiamo la libreria per verificarne l'installazione e utilizzarla in seguito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alcuni algoritmi richiedono dati o modelli di conoscenza non distribuiti di default con la libreria (per limitarne le dimensioni)\n",
    "- Dove necessari, viene usata la funzione `download` per scaricare i dati se non già presenti su disco\n",
    "  - i dati scaricati sono salvati in una directory `nltk_data` nella propria home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Segmentazione\n",
    "\n",
    "- La _segmentazione_ (_tokenization_) consiste nella scomposizione di un testo in una **sequenza di elementi** (_token_)\n",
    "- Comunemente la segmentazione è usata per estrarre le **singole parole** da un testo, includendo opzionalmente numeri, segni di punteggiatura, ...\n",
    "- NLTK offre la funzione `word_tokenize` per scomporre una stringa in una lista di parole e segni di punteggiatura\n",
    "- La funzione utilizza un modello della lingua inglese per scomporre correttamente alcune parole\n",
    "- Usiamo la funzione `download` per scaricare tale modello (se non già scaricato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Federico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Sia data una frase d'esempio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This isn't an example, or is it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...possiamo utilizzare il metodo `split` di Python per suddividere la frase in parole separate dagli spazi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This    isn't    an    example,    or    is    it?\n"
     ]
    }
   ],
   "source": [
    "words = sentence.split()\n",
    "# ottengo una lista di stringhe\n",
    "# uso join per stamparla isolando le singole parole\n",
    "print(\"    \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usando però `word_tokenize`, grazie alla conoscenza della lingua, sono correttamente separati segni di punteggiatura e anche parole composte come \"isn't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This    is    n't    an    example    ,    or    is    it    ?\n"
     ]
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(sentence)\n",
    "print(\"    \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of Words e Vector Space Model\n",
    "\n",
    "- Nel modello _Bag of Words_ (BoW), un testo è rappresentato dall'**insieme delle parole in esso**\n",
    "  - non si considera il loro ordine nella frase\n",
    "- Definito un dizionario $D$ di parole distinte, possiamo rappresentare un testo (_documento_) con un vettore che associ ad ogni parola in $D$ il numero di occorrenze in esso\n",
    "- Il _Vector Space Model_ prevede di rappresentare un insieme di documenti in uno **spazio vettoriale** comune, dove **le dimensioni corrispondono ai termini** di un dizionario comune\n",
    "- Un insieme di documenti in uno spazio vettoriale è rappresentabile con una **_matrice documenti-termini_**, di cui ogni riga costituisce il vettore ricavato dal documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definire uno spazio vettoriale\n",
    "\n",
    "- Dato un insieme di documenti di testo (in questo caso semplici frasi)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"the sky is blue\",\n",
    "    \"sky is blue and sky is beautiful\",\n",
    "    \"the beautiful sky is so blue\",\n",
    "    \"i love blue cheese\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possiamo usare il filtro `CountVectorizer` fornito da scikit-learn per rappresentarli in uno spazio vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Con `fit_transform` costruiamo lo spazio vettoriale sulla base dei termini presenti nei documenti e otteniamo la matrice documenti-termini che li rappresenta\n",
    "  - scikit-learn include un algoritmo basilare per segmentare le parole, usato di default da `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vect.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'oggetto `dtm` ottenuto è una _matrice sparsa_, una struttura dati che rappresenta una matrice memorizzando in modo esplicito solamente i valori diversi da 0\n",
    "  - in applicazioni reali tipiche, una matrice documenti-termini contiene meno del 10% di valori diversi da 0, si ottiene così un grande risparmio di memoria\n",
    "- **Attenzione:** la matrice sparsa è simile ad un `ndarray` ma con alcune differenze, ad es. l'operatore `*`  esegue il prodotto canonico tra matrici piuttosto che quello elemento per elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per visualizzare i valori della matrice, possiamo convertirla in un array NumPy col metodo `toarray`\n",
    "  - Su questa matrice piccola non ci sono problemi di memoria. **Usare con cautela su matrici più grandi!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 2, 0, 2, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Per ottenere l'elenco dei termini nel dizionario costruito, usare il metodo `get_feature_names` (o `get_feature_names_out` in versioni recenti)\n",
    "  - di default, ci sono tutte le parole distinte presenti nei documenti\n",
    "  - le parole sono ordinate in modo concorde con le colonne della matrice documenti-termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Possiamo visualizzare la matrice in un frame, esplicitando documenti e termini a cui si riferiscono righe e colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the sky is blue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky is blue and sky is beautiful</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the beautiful sky is so blue</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i love blue cheese</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  and  beautiful  blue  cheese  is  love  sky  \\\n",
       "the sky is blue                     0          0     1       0   1     0    1   \n",
       "sky is blue and sky is beautiful    1          1     1       0   2     0    2   \n",
       "the beautiful sky is so blue        0          1     1       0   1     0    1   \n",
       "i love blue cheese                  0          0     1       1   0     1    0   \n",
       "\n",
       "                                  so  the  \n",
       "the sky is blue                    0    1  \n",
       "sky is blue and sky is beautiful   0    0  \n",
       "the beautiful sky is so blue       1    1  \n",
       "i love blue cheese                 0    0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    dtm.toarray(),\n",
    "    index=docs,\n",
    "    columns=vect.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Col metodo `transform`, possiamo rappresentare ulteriori documenti nel medesimo spazio vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"loving this blue sky today\"\n",
    "vect.transform([new_doc]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loving this blue sky today</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            and  beautiful  blue  cheese  is  love  sky  so  \\\n",
       "loving this blue sky today    0          0     1       0   0     0    1   0   \n",
       "\n",
       "                            the  \n",
       "loving this blue sky today    0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    vect.transform([new_doc]).toarray(),\n",
    "    index=[new_doc],\n",
    "    columns=vect.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si noti che alcune parole del nuovo documento (es. \"loving\") si perdono nella trasformazione, in quanto non presenti nei documenti su cui è stato costruito lo spazio vettoriale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caso di studio: Classificazione di recensioni\n",
    "\n",
    "- Sul Web sono continuamente pubblicate opinioni degli utenti, ad es. di film\n",
    "  - alcune di queste (es. su Amazon) sono etichettate con un numero di stelle, che indicano se sia positiva o negativa\n",
    "  - su altre (es. messaggi sui forum) non abbiamo tale informazione strutturata, ma solo il testo\n",
    "- Vogliamo addestrare un classificatore su recensioni etichettate come positive o negative, in modo che sia in grado di stimare l'orientamento di opinioni non etichettate\n",
    "- Utilizziamo un file di 10000 recensioni di film tratte da Amazon, a ciascuna delle quali è associato un punteggio da 1 a 5 stelle\n",
    "- Utilizzando il vector space model, possiamo addestrare un modello sui conteggi di tutte le parole presenti nei documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"reviews.csv.gz\"):\n",
    "    from urllib.request import urlretrieve\n",
    "    urlretrieve(\"https://git.io/fj4cS\", \"reviews.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Carichiamo il file come DataFrame con `read_csv`\n",
    "  - il file è compresso col formato GZIP, riconosciuto automaticamente tramite l'estensione \".gz\" (se non riconosciuto, ad es. perché si usa l'URL, specificare `compression=\"gzip\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"reviews.csv.gz\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il file contiene due colonne\n",
    "  - in `text` troviamo il testo della recensione\n",
    "  - in `stars` troviamo il numero di stelle date dall'utente, da 1 a 5\n",
    "- Visioniamo alcune righe del dataset, aumentando prima il numero di caratteri visualizzati per parola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>Survivor series 2003 had alot on the line including jobs,titles and lives in the burried alive m...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Dudley Boyz VS Ric Flair and Batista: This match was rather short. Dudley's looked good but Coac...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>You seen one heist film, you seen them all. But every once in a while, somebody who really gives...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Often compared with \"The Big Chill\", and getting numerous stars in many reviews, this film simpl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This collection of Laurel and Hardy films contains five total selections.  Four of these are sho...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>I love Vin Diesel but I wish I'd skipped this movie. The first bad sign was the fact that this t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>When The Office was first shown to a UK audience back in 2001, it was shown on BBC2. That is the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "9993  Survivor series 2003 had alot on the line including jobs,titles and lives in the burried alive m...   \n",
       "9994  Dudley Boyz VS Ric Flair and Batista: This match was rather short. Dudley's looked good but Coac...   \n",
       "9995  You seen one heist film, you seen them all. But every once in a while, somebody who really gives...   \n",
       "9996  Often compared with \"The Big Chill\", and getting numerous stars in many reviews, this film simpl...   \n",
       "9997  This collection of Laurel and Hardy films contains five total selections.  Four of these are sho...   \n",
       "9998  I love Vin Diesel but I wish I'd skipped this movie. The first bad sign was the fact that this t...   \n",
       "9999  When The Office was first shown to a UK audience back in 2001, it was shown on BBC2. That is the...   \n",
       "\n",
       "      stars  \n",
       "9993      4  \n",
       "9994      4  \n",
       "9995      5  \n",
       "9996      1  \n",
       "9997      3  \n",
       "9998      3  \n",
       "9999      5  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 1: Analisi esplorativa\n",
    "\n",
    "- **(1a)** Ottenere il numero di recensioni per ciascun numero di stelle\n",
    "- **(1b)** Disegnare un grafico a torta con la distribuzione del numero di stelle\n",
    "- **(1c)** Disegnare un'istogramma con la distribuzione del numero di caratteri nelle recensioni\n",
    "  - sia `X` una serie pandas di stringhe, `X.str.len()` restituisce una serie col numero di caratteri di ciascuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    4708\n",
       "4    2620\n",
       "3    1434\n",
       "2     704\n",
       "1     534\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"stars\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews[\"stars\"].value_counts().plot.pie();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAFlCAYAAADVmk8OAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYCElEQVR4nO3dcdBldXkf8O/jrgFMJEpZ7c4uZiFDTdAxCiulY0wbSSqKiqY13UxbmcaGxpAZnbRTF3US+wcz2E5MyqRqsLGCsSImUWmUaZBEnc4QcVEUASlrQNlAYWOmBRMHAz794z1rLsu7y93l/e373nc/n5k799znnnPv733m7J3vnvu751R3BwAAWFlPWu0BAADAeiRoAwDAAII2AAAMIGgDAMAAgjYAAAwgaAMAwAAbV3sAo5x44om9bdu21R4GAADr2I033vgX3b1puefWbdDetm1bdu3atdrDAABgHauqrx/oOVNHAABgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBggI2rPYD1ZtvOT6zae991ybmr9t4AADyaI9oAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMMD9pVtaGqvlhVfzg9PqGqrq2qO6b7p8+se1FV7a6q26vqpTP1M6rq5um5S6uqRo8bAACeiCNxRPuNSW6bebwzyXXdfWqS66bHqarTkuxI8pwk5yR5V1VtmLZ5d5ILkpw63c45AuMGAIDDNjRoV9XWJOcm+a8z5fOSXD4tX57k1TP1K7v7oe6+M8nuJGdW1eYkx3f39d3dSa6Y2QYAANak0Ue0fzPJv0/y3ZnaM7v73iSZ7p8x1bckuXtmvT1Tbcu0vH/9MarqgqraVVW79u7duyJ/AAAAHI5hQbuqXpHk/u6+cd5Nlqn1QeqPLXZf1t3bu3v7pk2b5nxbAABYeRsHvvaLkryqql6e5Ngkx1fV7ya5r6o2d/e907SQ+6f19yQ5aWb7rUnumepbl6kDAMCaNeyIdndf1N1bu3tbln7k+Mfd/S+SXJ3k/Gm185N8fFq+OsmOqjqmqk7O0o8eb5imlzxYVWdNZxt53cw2AACwJo08on0glyS5qqpen+QbSV6bJN19S1VdleTWJA8nubC7H5m2eUOS9yc5Lsk10w0AANasIxK0u/vTST49LX8zydkHWO/iJBcvU9+V5LnjRggAACvLlSEBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBggGFBu6qOraobqupLVXVLVf2HqX5CVV1bVXdM90+f2eaiqtpdVbdX1Utn6mdU1c3Tc5dWVY0aNwAArISRR7QfSvKS7v6xJM9Pck5VnZVkZ5LruvvUJNdNj1NVpyXZkeQ5Sc5J8q6q2jC91ruTXJDk1Ol2zsBxAwDAEzYsaPeSb00PnzzdOsl5SS6f6pcnefW0fF6SK7v7oe6+M8nuJGdW1eYkx3f39d3dSa6Y2QYAANakoXO0q2pDVd2U5P4k13b355I8s7vvTZLp/hnT6luS3D2z+Z6ptmVa3r8OAABr1tCg3d2PdPfzk2zN0tHp5x5k9eXmXfdB6o99gaoLqmpXVe3au3fvIY8XAABWyhE560h3/98kn87S3Or7pukgme7vn1bbk+Skmc22Jrlnqm9dpr7c+1zW3du7e/umTZtW8k8AAIBDMvKsI5uq6mnT8nFJfirJV5NcneT8abXzk3x8Wr46yY6qOqaqTs7Sjx5vmKaXPFhVZ01nG3ndzDYAALAmbRz42puTXD6dOeRJSa7q7j+squuTXFVVr0/yjSSvTZLuvqWqrkpya5KHk1zY3Y9Mr/WGJO9PclySa6YbAACsWcOCdnd/OckLlql/M8nZB9jm4iQXL1PfleRg87sBAGBNcWVIAAAYQNAGAIABBG0AABhA0AYAgAEEbQAAGEDQBgCAAQRtAAAYQNAGAIABBG0AABhA0AYAgAEEbQAAGEDQBgCAAQRtAAAYQNAGAIAB5graVfXc0QMBAID1ZN4j2u+pqhuq6peq6mkjBwQAAOvBxnlW6u4fr6pTk/x8kl1VdUOS/9bd1w4dHYdk285PrMr73nXJuavyvgAAa9ncc7S7+44kb0vy5iT/MMmlVfXVqvqZUYMDAIBFNe8c7edV1W8kuS3JS5K8srt/dFr+jYHjAwCAhTTX1JEkv5XkvUne0t3f3lfs7nuq6m1DRgYAAAts3qD98iTf7u5HkqSqnpTk2O7+6+7+wLDRAQDAgpp3jvankhw38/gpUw0AAFjGvEH72O7+1r4H0/JTxgwJAAAW37xB+6+q6vR9D6rqjCTfPsj6AABwVJt3jvabknykqu6ZHm9O8s+GjAgAANaBeS9Y8/mq+pEkz05SSb7a3X8zdGQAALDA5j2inSQvTLJt2uYFVZXuvmLIqAAAYMHNFbSr6gNJfjjJTUkemcqdRNAGAIBlzHtEe3uS07q7Rw4GAADWi3nPOvKVJH935EAAAGA9mfeI9olJbq2qG5I8tK/Y3a8aMioAAFhw8wbtt48cBAAArDfznt7vM1X1Q0lO7e5PVdVTkmwYOzQAAFhcc83RrqpfSPJ7SX57Km1J8rFBYwIAgIU3748hL0zyoiQPJEl335HkGaMGBQAAi27eoP1Qd39n34Oq2pil82gDAADLmDdof6aq3pLkuKr66SQfSfI/xg0LAAAW27xBe2eSvUluTvJvknwyydtGDQoAABbdvGcd+W6S9043AADgccwVtKvqziwzJ7u7T1nxEQEAwDow7wVrts8sH5vktUlOWPnhAADA+jDXHO3u/ubM7c+7+zeTvGTs0AAAYHHNO3Xk9JmHT8rSEe6nDhkRAACsA/NOHfn1meWHk9yV5GdXfDQAALBOzHvWkZ8cPRAAAFhP5p068isHe76737kywwEAgPXhUM468sIkV0+PX5nks0nuHjEoAABYdPMG7ROTnN7dDyZJVb09yUe6+1+PGhgAACyyeS/B/qwk35l5/J0k21Z8NAAAsE7Me0T7A0luqKqPZukKka9JcsWwUQEAwIKb96wjF1fVNUlePJX+VXd/cdywAABgsc07dSRJnpLkge7+z0n2VNXJg8YEAAALb66gXVW/luTNSS6aSk9O8rujBgUAAItu3iPar0nyqiR/lSTdfU9cgh0AAA5o3qD9ne7uLP0QMlX1/eOGBAAAi2/eoH1VVf12kqdV1S8k+VSS944bFgAALLbHPetIVVWSDyf5kSQPJHl2kl/t7msHjw0AABbW4wbt7u6q+lh3n5FEuAYAgDnMO3XkT6vqhUNHAgAA68i8V4b8ySS/WFV3ZenMI5Wlg93PGzUwAABYZAc9ol1Vz5oWX5bklCQvSfLKJK+Y7g+27UlV9SdVdVtV3VJVb5zqJ1TVtVV1x3T/9JltLqqq3VV1e1W9dKZ+RlXdPD136TRvHAAA1qzHmzrysSTp7q8neWd3f3329jjbPpzk33b3jyY5K8mFVXVakp1JruvuU5NcNz3O9NyOJM9Jck6Sd1XVhum13p3kgiSnTrdzDu3PBACAI+vxgvbskeNTDuWFu/ve7v7CtPxgktuSbElyXpLLp9UuT/Lqafm8JFd290PdfWeS3UnOrKrNSY7v7uunc3lfMbMNAACsSY8XtPsAy4ekqrYleUGSzyV5ZnffmyyF8STPmFbbkuTumc32TLUt0/L+9eXe54Kq2lVVu/bu3Xu4wwUAgCfs8YL2j1XVA1X1YJLnTcsPVNWDVfXAPG9QVT+Q5PeTvKm7D7bNcvOu+yD1xxa7L+vu7d29fdOmTfMMDwAAhjjoWUe6e8PBnn88VfXkLIXsD3b3H0zl+6pqc3ffO00LuX+q70ly0szmW5PcM9W3LlMHAIA1a97zaB+y6cwgv5Pktu5+58xTVyc5f1o+P8nHZ+o7quqYqjo5Sz96vGGaXvJgVZ01vebrZrYBAIA1ad7zaB+OFyX5l0lurqqbptpbklyS5Kqqen2SbyR5bZJ09y1VdVWSW7N0xpILu/uRabs3JHl/kuOSXDPdAABgzRoWtLv7f2X5+dVJcvYBtrk4ycXL1Hclee7KjQ4AAMYaNnUEAACOZoI2AAAMIGgDAMAAgjYAAAwgaAMAwACCNgAADCBoAwDAAII2AAAMIGgDAMAAgjYAAAwgaAMAwACCNgAADCBoAwDAAII2AAAMIGgDAMAAgjYAAAwgaAMAwACCNgAADCBoAwDAAII2AAAMIGgDAMAAgjYAAAwgaAMAwACCNgAADCBoAwDAAII2AAAMIGgDAMAAgjYAAAwgaAMAwACCNgAADCBoAwDAAII2AAAMIGgDAMAAgjYAAAwgaAMAwACCNgAADCBoAwDAABtXewAsvm07P7Eq73vXJeeuyvsCAMzDEW0AABhA0AYAgAEEbQAAGEDQBgCAAQRtAAAYQNAGAIABBG0AABhA0AYAgAEEbQAAGEDQBgCAAQRtAAAYQNAGAIABBG0AABhA0AYAgAEEbQAAGEDQBgCAAQRtAAAYQNAGAIABBG0AABhA0AYAgAEEbQAAGEDQBgCAAYYF7ap6X1XdX1VfmamdUFXXVtUd0/3TZ567qKp2V9XtVfXSmfoZVXXz9NylVVWjxgwAACtl5BHt9yc5Z7/aziTXdfepSa6bHqeqTkuyI8lzpm3eVVUbpm3eneSCJKdOt/1fEwAA1pxhQbu7P5vkL/crn5fk8mn58iSvnqlf2d0PdfedSXYnObOqNic5vruv7+5OcsXMNgAAsGYd6Tnaz+zue5Nkun/GVN+S5O6Z9fZMtS3T8v71ZVXVBVW1q6p27d27d0UHDgAAh2Kt/BhyuXnXfZD6srr7su7e3t3bN23atGKDAwCAQ3Wkg/Z903SQTPf3T/U9SU6aWW9rknum+tZl6gAAsKYd6aB9dZLzp+Xzk3x8pr6jqo6pqpOz9KPHG6bpJQ9W1VnT2UZeN7MNAACsWRtHvXBVfSjJP0pyYlXtSfJrSS5JclVVvT7JN5K8Nkm6+5aquirJrUkeTnJhdz8yvdQbsnQGk+OSXDPdAABgTRsWtLv75w7w1NkHWP/iJBcvU9+V5LkrODQAABhurfwYEgAA1hVBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABhC0AQBgAEEbAAAGELQBAGAAQRsAAAYQtAEAYABBGwAABti42gOAw7Vt5ydW7b3vuuTcVXtvAGAxOKINAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADCAoA0AAAMI2gAAMICgDQAAAwjaAAAwgKANAAADCNoAADDAxtUeACyibTs/sSrve9cl567K+wIAh84RbQAAGEDQBgCAAQRtAAAYQNAGAIABBG0AABhA0AYAgAEEbQAAGEDQBgCAAQRtAAAYQNAGAIABXIIdFshqXfo9cfl3ADhUjmgDAMAAgjYAAAxg6ggwl9WatmLKCgCLyhFtAAAYQNAGAIABBG0AABhgYYJ2VZ1TVbdX1e6q2rna4wEAgINZiB9DVtWGJP8lyU8n2ZPk81V1dXffurojA0Zz7nAAFtVCBO0kZybZ3d1/liRVdWWS85II2sAwzrQCwBOxKEF7S5K7Zx7vSfL3V2ksAEOt5lH8o43/1AAjLUrQrmVq/ZiVqi5IcsH08FtVdfvQUS3vxCR/sQrvezTQ2zH0dRy9HWdFelvvWIGRrD/22zH0dZzV7u0PHeiJRQnae5KcNPN4a5J79l+puy9LctmRGtRyqmpXd29fzTGsV3o7hr6Oo7fj6O04ejuGvo6zlnu7KGcd+XySU6vq5Kr6viQ7kly9ymMCAIADWogj2t39cFX9cpL/mWRDkvd19y2rPCwAADighQjaSdLdn0zyydUexxxWderKOqe3Y+jrOHo7jt6Oo7dj6Os4a7a31f2Y3xQCAABP0KLM0QYAgIUiaK8Ql4g/dFV1UlX9SVXdVlW3VNUbp/rbq+rPq+qm6fbymW0umnp8e1W9dKZ+RlXdPD13aVUtd0rIo0ZV3TX146aq2jXVTqiqa6vqjun+6TPr6+scqurZM/vlTVX1QFW9yT57eKrqfVV1f1V9Zaa2YvtpVR1TVR+e6p+rqm1H9A9cRQfo7X+qqq9W1Zer6qNV9bSpvq2qvj2z/75nZhu93c8BertinwFHa28P0NcPz/T0rqq6aaovzj7b3W5P8JalH2h+LckpSb4vyZeSnLba41rrtySbk5w+LT81yf9OclqStyf5d8usf9rU22OSnDz1fMP03A1J/kGWzrl+TZKXrfbft8q9vSvJifvV/mOSndPyziTv0Ncn1OMNSf5Pls6fap89vB7+RJLTk3xlprZi+2mSX0rynml5R5IPr/bfvMq9/cdJNk7L75jp7bbZ9fZ7Hb2dr7cr9hlwtPZ2ub7u9/yvJ/nVaXlh9llHtFfG9y4R393fSbLvEvEcRHff291fmJYfTHJblq4CeiDnJbmyux/q7juT7E5yZlVtTnJ8d1/fS/+Crkjy6rGjX0jnJbl8Wr48f9sjfT08Zyf5Wnd//SDr6O1BdPdnk/zlfuWV3E9nX+v3kpx9tHxzsFxvu/uPuvvh6eGfZumaFAekt8s7wH57IPbbOR2sr9Pf/7NJPnSw11iLfRW0V8Zyl4g/WGBkP9NXOC9I8rmp9MvT15vvm/nq+EB93jIt718/mnWSP6qqG2vpiqlJ8szuvjdZ+k9OkmdMdX09PDvy6A99++zKWMn99HvbTAHz/yX5O8NGvlh+PktH+/Y5uaq+WFWfqaoXTzW9PTQr9Rmgt4/14iT3dfcdM7WF2GcF7ZUx1yXiWV5V/UCS30/ypu5+IMm7k/xwkucnuTdLXxclB+6z/j/Wi7r79CQvS3JhVf3EQdbV10NUSxfOelWSj0wl++x4h9NLfV5GVb01ycNJPjiV7k3yrO5+QZJfSfLfq+r46O2hWMnPAL19rJ/Low9sLMw+K2ivjLkuEc9jVdWTsxSyP9jdf5Ak3X1fdz/S3d9N8t4sTc1JDtznPXn0V6BHff+7+57p/v4kH81SD++bvlbb9/Xa/dPq+nroXpbkC919X2KfXWEruZ9+b5uq2pjkBzP/V/7rUlWdn+QVSf759NV6pmkN35yWb8zSPOK/F72d2wp/BujtjKkHP5Pkw/tqi7TPCtorwyXiD8M0N+p3ktzW3e+cqW+eWe01Sfb9AvnqJDumXw6fnOTUJDdMXy8/WFVnTa/5uiQfPyJ/xBpUVd9fVU/dt5ylH0B9JUv9O39a7fz8bY/09dA96uiKfXZFreR+Ovta/zTJH+8Ll0ejqjonyZuTvKq7/3qmvqmqNkzLp2Spt3+mt/Nb4c8AvX20n0ry1e7+3pSQhdpnj8QvLo+GW5KXZ+msGV9L8tbVHs8i3JL8eJa+tvlykpum28uTfCDJzVP96iSbZ7Z569Tj2zNzloYk27P0wfa1JL+V6WJMR+MtS2e/+dJ0u2Xf/piluWjXJbljuj9BXw+rv09J8s0kPzhTs88eXi8/lKWvgP8mS0ebXr+S+2mSY7M0vWd3ls5EcMpq/82r3NvdWZqjuu/zdt8ZGP7J9FnxpSRfSPJKvT3k3q7YZ8DR2tvl+jrV35/kF/dbd2H2WVeGBACAAUwdAQCAAQRtAAAYQNAGAIABBG0AABhA0AYAgAEEbQAAGEDQBgCAAQRtAAAY4P8DFeqZHWOoIYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews[\"text\"].str.len().plot.hist(bins=20, figsize=(12,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 2: Suddivisione in recensioni positive e negative\n",
    "\n",
    "- Per semplificare l'analisi, riconduciamo i 5 possibili numeri di stelle a due classi\n",
    "- **(2a)** Aggiungere al frame `reviews` una colonna `label` che associ ad ogni recensione la stringa `\"pos\"` se ha 4 o 5 stelle, `\"neg\"` altrimenti\n",
    "  - consiglio: usare `np.where`\n",
    "- **(2b)** Visualizzare il numero di valori `\"pos\"` e `\"neg\"` nella colonna `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"label\"] = np.where(reviews[\"stars\"]>=4,\"pos\",\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    7328\n",
       "neg    2672\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAADnCAYAAADGrxD1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUc0lEQVR4nO3de5AdZZnH8e8zuQyEECGERCKXToZIChABRQURFXaX1UYRVxTMKiiIrKyCINjquh51rWopXMQbsqAorKKobMHSiOIWArJyDQiKCwTTKzKDkws5uRAmCfPuH91ZUuNk5pyZ0/10v+f5VJ2aTCaZ95fM/Obty9vd4pzDGOOPHu0AxpjOslIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGesVIb4xkrtTGemaodwHReECVTgQVAH/BiYHdg7jZv5wA7AlNGeW0CmsCaEW9XACnwB+APaRwOlvTPMW0Se0BevQVREgCvAV4GHADsT1boon9grweWA48BS4F7gfvSOHym4HHNOKzUNRJEyRTgYOC1wJH52/mamUZwwBNkBb8buCWNw0d0I3UfK3XFBVGyG3AccDzw18BM3URt+yPwM+CnwC/SOFynnMd7VuoKCqJkAfA2siIfSbav64PNwK+Aa4AfpXG4RjeOn6zUFRFEyQzgncDpZJvVvhsCbgSuBm5K43Czch5vWKmVBVHySrIinwzMUo6jZRXZ7P21NA4f1Q5Td1ZqBfkBrxOB84FDleNUiQNuAS5O4/Bm7TB1ZaUuURAl04FTgAuAfZXjVN3DwEXANbZp3h4rdQny/eUzgXOBlyjHqZsngH8CfpjGoX2ztsBKXaAgSoRsZv4C1TqfXEdLgU+kcfhz7SBVZ6UuSBAlRwEXY/vMnfZfQJTG4X3aQarKSt1hQZT0ARcCb9fO4jEHXEZW7qZ2mKqxUndIfkT7POCzwA7KcbrFAPCRNA5/rB2kSrwotYgEwM1k640PIbvI4L3A4WRHUKeSrUf+B+fckIjEwFuBLcDPnXMfm8z4QZQsBr4DvHoyn8dM2A3AWWkc/kk7SBX4VOrlwJHOuTtF5Ntklwh+EDjGOfeYiFxFdrDlKuDXwGLnnBORXZxzayYybj47fwxoYLOztvVks/aV2kG0+VTq251ze+fvHw18GpjinDsq/71jgLPIlmLeD9wHJMCNzrlN7Y6Zr8++Bpudq+Z7wJlpHK7XDqLFpzuftPTTyTm3BXgV8BOyiybaXrkURElI9oPBCl09S4ClQZQcoh1Ei08z9XLgCOfcr0XkcrK7dHwQONo5t0xEvgM8AHwLmOGcGxSR2cAy59zsVsYJoqSH7EDYpwDp+D/EdNIQcEEah1/RDlI2n0p9E3A7cATwOPAeRjlQBswGrifbBxbgIufcd8cbI4iSOcD3ya5pNvVxNXB6Godt72LVlU+lvtE5d2ARnz+IkgPJ9r/3LuLzm8LdDpyQxuFq7SBl8GmfuhBBlBxDdmG/Fbq+jgLuCqJkkXaQMngxUxcliJIlwJXANO0spiNWkc3Yd2gHKZLN1NsRRMnZZPtjVmh/7Abckp+98JaVehRBlHwG+DJ2hNtHvcB1QZS8RTtIUazUIwRR8kmyFWLGX9OBHwdR8jbtIEWwfeptBFFyDtnlkqY7bAZOSuPwOu0gnWSlzgVRciZwqXYOU7otwDvSOLxeO0inWKmBIEpOITvKbfvQ3Wkj8MY0Du/WDtIJXV/qIEqOJVtY4ssN883ErAAOT+PwCe0gk9XVpc6vg74LeJF2FlMJjwNHpHG4UjvIZHTt0e8gSmaTPSHCCm22WgTcEERJra+N78pSB1EyjezSyz7tLKZyDgeu0A4xGV1ZauBrwBu0Q5jKWhJEyRnaISaq6/ap8/Xc/66dw1Tec2QHzh7UDtKurip1fguiB+neB9GZ9jwOvDKNw7XaQdrRNZvfQZRMJbvJgRXatGoRNdy/7ppSA58BXqMdwtTOiUGUnK4doh1dsfkdRMnrgF/SXT/ETOesBQ5M4/BJ7SCt8P6bPIiSXrKbDXr/bzWFmQVcrh2iVd3wjR6R7RsZMxnHBlHyHu0QrfB68zuIkn2B35JdGG/MZK0EFqdxuEo7yFh8n6m/gRXadM4c4IvaIcbj7UwdRMm7gB9o5zDeGQYOSePwIe0g2+PlTJ0vyP+Sdg7jpR6y549XlpelBj4EvEQ7hPHWsUGUVPZJLd5tfgdRsjPZY2znaGcxXnsQeEUah8PaQUbycaY+Byu0Kd7BwN9rhxiNVzN1ECW7kj390m58YMqwDNivarO1bzP1+VihTXn2BU7QDjGSN6UOomQnskfVGlOmC7QDjORNqYFTgF20Q5iu86ogSl6vHWJbXpQ6iBIBPqydw3StSs3WXpQa+BtgsXYI07XeFETJ/tohtvKl1GdrBzBdTYD3a4fYqvantIIoWUh2asEemWM0/RnYM43DLdpBfJipl2CFNvrmAW/SDgF+lPpk7QDG5E7VDgA13/wOouRg4AHtHMbkNgHztW+iUPeZ2mZpUyXTgXdph6htqfNz0ydp5zBmhLdqB6htqcnu4b23dghjRnhDvmRZTZ1L/WbtAMaMohf4K80AdS7132oHMGY7jtMcvJZHv4MomQMMYuenTTX1ky1EUSlXXWfqN2KFNtU1HzhIa/C6lvpo7QDGjOMIrYHrWuqjtAMYMw61J6zWrtT56QK7zNJUnZW6DQdTz9ymuywKomS2xsB1LMch2gGMaYGgNFvXsdSHagcwpkWHaQxqpTamOCrHfmpV6iBKpgGVuReUMeNYpDForUoN7AVM0w5hTIus1C3YRzuAMW2YFUTJ3LIHrVup7VJLUzelz9Z1K7XN1KZuSi/11LE+KCJvH+vjzrnrOhtnXFZqUzfzyh5wzFIDbxnjYw4ou9S2+W3qpvRVZWOW2jn3vrKCtGg37QDGtKn079mW9qlFZJ6IfEtEfpq/v7+InFZstFHNVBjTmMkofaZu9UDZd4CfkV38DfAYcE4BecZjpTZ1U82ZGpjjnLsWGAZwzm0Bni8s1fZZqU3dVHam3iAiu5EdHENEXgM0C0s1ivw+36q3XjVmAmaUPeB4R7+3Ohe4AegTkTuB3YF3FJZqdDOo33l1Y6aUPWBLpXbOLRWR1wP7kV0n+qhzbnOhyf5S6f85xnRA6RNRS6UWkR2ADwFHkm2C3yEi33TOPVdkuBHK/iHSNfaSwadum/7RHbVz+GgYacIzpY7Z6ub3VcA64Kv5+ycDVwMnFhFqO4ZKHKurLJT+1T3iXqadw0c9uFKPPWVjtmY/59xpzrlb89cZwEuLDDZSGofD6Bxx996+0r9eO4PHtpQ9YKulfiA/4g2AiLwauLOYSGPapDCm9/qk3/5fi1P6buN4F3Q8TLYPPQ14r4j8MX9/H+CR4uP9hU2A7ft1WCBPa0fwWek/MMfbp1Z90NcoNgAv0g7hm/myqlc7g8cGyx5wvAs6/nfb90VkLrBDoYnG9jQvLFU1HTJb1tpKveI8VfaArV7Q8VYReRxYDtwGpMBPC8y1PbadWIAZDNnVb8WpZqmBz5PdmPwx59wC4Bh0DpQNKIzpNWF4eArDu2vn8FhlS73ZObcK6BGRHufcrWSPvymbzdQd9mKeWSHS8noF077SS93qF3ONiMwEbge+JyKDKJx/w2bqjtun5+lVKNxyp4tUdqY+HtgIfBS4GXiCsW91VJTS/4N81ycD67QzeK6aM7VzbsM27363oCyt+L3i2F7qk/4y1+93m81U7ZSWiKwjv4Z65IcA55ybVUiq7VtGtgbczqt2yAKxPZoC9dNojtafQo25+e2c29k5N2uU184KhSaNw+eBR8se12d7ykp7jFFxfqMxaB1vOvBb7QA+mSNNu5tMce7WGLSOpf6ddgCfzGTjrtoZPHaPxqB1LPXD2gF8MpXnS3+AW5dwwL0aA9ex1L9m9IN3pk2zWbtaRHUtv88epdEs/QYJUMNSp3G4Eju11RH7yJ9XamfwmMr+NNSw1LnbtAP4oK+nX2Um6RIq+9NQ31Lfrh3AB33Sv1E7g8dspm6TlboDFsiAHZsoxrPAQ1qD17LUaRz2A49r56i7vWSF3Uu9GDfTaKrd0rqWpc7dpB2g7ubKmtIfCdMlfqI5eJ1Lfb12gLqbxYZdtDN4aAi4UTNAnUt9B7BaO0SdTWeL3fGk826h0VyrGaC2pU7jcAs2W0/YTJ5dJ8LO2jk8pLrpDTUude5a7QB1tbcMln6dbxfYTAUmmrqX+hfAKu0QdbRQBmzhSefdSqNZ7tPwRlHrUueb4Fdp56ijPul/VjuDh9Q3vaHmpc5dph2gjhb29NvDBjtrCLhOOwR4UOo0Dh8Ffqmdo272kcHaf+0r5oc0mpW4QMaXL6zN1m2aJ6vtksvO+ur4f6QcvpT6OmCFdog62YUN9qDBzrmLRvM+7RBbeVHqNA43AVdo56iTXjbP0c7gka9oB9iWF6XOfZnsgQNmHL1seq5H3GztHJ5Iqdh6CW9KncbhILZv3ZI9ZYUtPOmcL9FoVupMgjelzl0I2BMnxrFABtQXSHhiBfAt7RAjeVXqNA4HqOB/ctX0ycB67QyeuIRGs3K7fF6VOhcDm7RDVFmf9Gs8sdQ3TwIXa4cYjXelTuPwT8Cl2jmqLOh5WrQzeODjNJqVXGrrXalzDaASq3uqaA9W2wMGJ+dXNJrXaIfYHi9LncbhGuBT2jmqaldZZ9dRT9wwcLZ2iLF4WercFcAD2iGqaEeGdtPOUGNX0mgu1Q4xFm9LncbhMPAR7RxVM4Xnt/Tg7DZGE7MW+KR2iPF4W2qANA5/BXxPO0eVzJdVgyJ+f90L9Hkazcov3OmGL+5HgAHtEFURyNN2s8aJ+T1wiXaIVnhf6jQOVwOna+eoioXSv047Qw0NASdr3qC/Hd6XGiCNw5uwq7gA2Ff6bWFO+86n0fyNdohWdUWpc+eSXVHT1QJ5WjtC3dxAo1mZGyC0omtKncbhOuBUsvOMXeslsnK6doYaeQp4v3aIdnVNqQHSOLyNLl+UspusnamdoSaGgSU0mrW7BXVXlRogjcOYitzKVcNOPLerdoaa+AKN5m3aISai60qdOxV4RDtE+ZybwvA87RQ1cCfwWe0QE9WVpU7jcD1wAtBVT6mYyzMrRZimnaPingDeXrW7mbSjK0sNkMbhY8ASoLZfvHYF8me7cm1sg8CxdVg1NpauLTVAGocJcKZ2jrL09djCkzGsB95Mo/mEdpDJ6upSA6RxeAXwCe0cZeiTAbt/2+g2k21y368dpBO6vtTw/0fEv6Sdo2gLZMBpZ6ggB5xKo3mLdpBOsVK/4Hzgu9ohirSnrJiqnaGCzqPR/L52iE6yUufSOHTAaYBXX+Bt7S7NnbQzVMwXaTQrefPAybBSbyONw+eB9wD/pp2lCDPZuIt2hgpp0GhG2iGKIM7ZbtZogii5kGyT3BvLe9/9rAgztHMoc8DZdbtIox02U29HGocXAJ/WztEpu7BujRWaLcApPhcarNRjSuPwX4B/xIMFKvvIYLc/6nct2Xnoq7WDFM1KPY40Dr8OHEf2TVFbC6W/q5bEjvAk8FqfTluNxUrdgjQObwYOB5ZpZ5movp7+yj3zqSRLgVfTaP5WO0hZrNQtSuPwEeAw4CbtLBOxUAa68eYQlwBH0Gh21Y0nrdRtyJ/88RbgM9RsP3tvGZyinaFEK4DjaDTPodEcavUviUggIr8XkctF5Hci8nMR2VFE+kTkZhG5X0TuEJHF+Z/vE5G7ROReEfmciFTiaaJW6jalcTicxuHngCOp0eb4XHlmR+0MJbkFOIhGM5ng318EfN05dwCwBvg7snULH3bOvQL4GPCN/M9eAlzinDsM6J9U6g6yUk9QGod3AQdTk7uUzuLZF2lnKNhm4ONkl05O5u6Ky51zD+a/vh8IgCOAH4nIg8BlwB75xw8HfpT/ujIrEW0t8CSkcbgB+EAQJTcClwOVfZxNL5vnamco0DLg3TSa93bgc227uf48MA9Y45w7uAOfuxQ2U3dAGofXAwcAV5KtWKqUndi4XoRZ2jkKMAR8ETikQ4UezVpguYicCCCZl+cfu4ts8xzgpILGb5uVukPSOFyRxuH7yTbVKvVUxL1khY8LT/4D2J9GM6LRLPoA1RLgNBH5DfA74Pj8988BzhWRe8g2ySuxFsDWfhcgiJIe4IPAFwD1u3e+qefuBy6dfskh2jk65CHgHBrNW7WDiMgMYKNzzonIScDJzrnjx/t7RbOZugD5EfJLgZcCFwOqCz/6pH+D5vgdspLs1lOHVqHQuVcAD4rIQ8CHgPOU8wA2U5ciiJI9gAg4A9ih7PEvnvb1206Ycufryx63Q54Fvkn2GNk1yllqwUpdoiBK5pM9tPx0oLesca+b/s93HNqz7HVljdch/cDXgMtoNO3xu22wUivIy31G/tpjnD8+af/d++F758uqw4oep0PuJ9tlubYuj46tGiu1oiBKppE9VOAs4Kiixnmk933/M0OGFhf1+TtgGLgB+FcazTu0w9Sdlboigig5EPgAcCIdnr3/0LtkZY+4OZ38nB2yHPghcIUP99uuCit1xeSnw44E3km2sOHFk/l809k89GjvKdNFkE7k64A/kS2t/AGN5j3aYXxkpa6wvOBHkS12OAY4ENor5wIZePLW3vP2KiBeqxzZYpz/JHuA+wOKWbqClbpGgijZHXgjcHT+WjTe3zm6Z+lD355+0UFFZ9vGBrIS35O/7qTRfKrE8bueXdBRI2kcrgCuzV8EUTIPeHn+Oih/uxheeLJlnwwUuYRyC/AwWXnvzd8+UucnRvrAZmrPBFEyHdiP7JLBvc+f+oOdzpp6wwFk++bz8tcsss34rS9GvC/Ac8AA2fnifuCpbX699fVkOzchMOWwUhvjGVv7bYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeMZKbYxnrNTGeOb/AHE0WQT/xt8rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews[\"label\"].value_counts().plot.pie();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classificazione di testi\n",
    "\n",
    "- La matrice documenti-termini ha la forma di un dataset\n",
    "  - ogni riga rappresenta un esempio (un testo) da classificare\n",
    "  - ogni colonna rappresenta una variabile che caratterizza gli esempi\n",
    "- Possiamo quindi addestrare un modello di classificazione su tale matrice per stimare l'orientamento delle recensioni\n",
    "- Iniziamo suddividendo come al solito i dati in training set (70\\%) e validation set (30\\%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "reviews_train, reviews_val = \\\n",
    "    train_test_split(reviews, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Definiamo quindi lo spazio vettoriale in cui rappresentare le recensioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Costruiamo lo spazio vettoriale sul training set (quindi con le parole contenute in esso) e otteniamone la matrice documenti-termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train = vect.fit_transform(reviews_train[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otteniamo uno spazio con tante dimensioni quante le parole distinte nelle recensioni di training, ovvero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51772"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Esempi di parole estratte sono..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abides',\n",
       " 'abiding',\n",
       " 'abigail',\n",
       " 'abilene',\n",
       " 'abilites',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abit',\n",
       " 'abition',\n",
       " 'abject']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La matrice è estremamente sparsa: la percentuale di termini non 0 è molto bassa\n",
    "  - posso vedere il numero e la percentuale dalle informazioni sulla matrice o stampando somma e media della matrice convertita a booleana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7000x51772 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 993179 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993179"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train.astype(bool).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002740529905850931"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train.astype(bool).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rappresentiamo i documenti del validation set nello stesso spazio vettoriale, ottenendone la corrispondente matrice documenti-termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_val = vect.transform(reviews_val[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- A questo punto creiamo un modello di classificazione e addestriamolo passando la matrice documenti-termini e le etichette delle recensioni relative al training set\n",
    "  - usiamo ad es. la regressione logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Federico\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lrm = LogisticRegression(solver=\"saga\", C=10)\n",
    "lrm.fit(dtm_train, reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usando matrice ed etichette del validation set possiamo valutare l'accuratezza del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.score(dtm_val, reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il modello assume `neg` come classe _negativa_ (-1) e `pos` come _positiva_ (1), come si deduce dall'ordine delle due nell'attributo `classes_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 3: Uso del modello\n",
    "\n",
    "- Consideriamo il seguente campione di due ipotetiche recensioni da classificare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = [\n",
    "    \"What an awesome movie!\",\n",
    "    \"It was really boring\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(3a)** Estrarre la loro rappresentazione nello spazio vettoriale costruito sui documenti di training\n",
    "- **(3b)** Ottenere le etichette previste dal modello per ciascuna\n",
    "- **(3c)** Ottenere le distribuzioni di probabilità tra le etichette date dal modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x51772 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_new = vect.transform(new_reviews)\n",
    "dtm_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'neg'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict(dtm_new) # predizione per ogni riga di new_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45242469, 0.54757531],\n",
       "       [0.54816952, 0.45183048]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.predict_proba(dtm_new) # predizione con stima della probabilità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parametri del modello\n",
    "\n",
    "- Possiamo accedere ai coefficienti lineari assegnati al modello per ciascuna variabile, ovvero per ciascun termine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00142716,  0.00540595,  0.00095471, -0.00021518])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.coef_[0, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possiamo creare una serie che associ a ciascuno di questi coefficienti il termine corrispondente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(lrm.coef_[0], index=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ordinando questa serie in base ai valori, vediamo quali sono i coefficienti più alti e più bassi e da questi **quali parole contribuiscano di più** a rendere una recensione positiva o negativa\n",
    "  - possiamo usare i metodi `nsmallest` o `nlargest` per selezionare gli N valori più bassi o più alti\n",
    "- I coefficienti più bassi fanno tendere la decisione alla classe \"-1\", in questo caso le recensioni negative..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad       -0.279635\n",
       "nothing   -0.204344\n",
       "worst     -0.183345\n",
       "plot      -0.180998\n",
       "just      -0.155710\n",
       "if        -0.151800\n",
       "boring    -0.137825\n",
       "but       -0.134179\n",
       "decent    -0.133992\n",
       "minutes   -0.133878\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.nsmallest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ...mentre quelli più alti fanno tendere alla classe \"1\", le positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "great        0.421359\n",
       "best         0.213177\n",
       "dvd          0.205943\n",
       "excellent    0.204526\n",
       "highly       0.180498\n",
       "love         0.180302\n",
       "well         0.179750\n",
       "you          0.177170\n",
       "very         0.166842\n",
       "season       0.155087\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 4: Pipeline per pre-processing testo e classificazione\n",
    "\n",
    "- Avendo `CountVectorizer` l'interfaccia standard dei filtri di scikit-learn, possiamo utilizzarlo in una pipeline\n",
    "- **(4a)** Definire una pipeline con un modello di regressione logistica applicato allo spazio vettoriale creato da un `CountVectorizer`, replicando la configurazione usata sopra\n",
    "- **(4b)** Addestrare il modello con le recensioni di training\n",
    "- **(4c)** Ricavare l'accuratezza del modello sulle recensioni di validation\n",
    "- **(4d)** Stampare le 5 parole con peso maggiore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vect\", CountVectorizer()),\n",
    "    (\"lreg\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Federico\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                ('lreg', LogisticRegression(C=10, solver='saga'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003333333333333"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad       -0.280439\n",
       "nothing   -0.203808\n",
       "worst     -0.182914\n",
       "plot      -0.181699\n",
       "just      -0.155825\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(\n",
    "    data = model.named_steps[\"lreg\"].coef_[0],\n",
    "    index = model.named_steps[\"vect\"].get_feature_names()\n",
    ")\n",
    "s.nsmallest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## tf.idf\n",
    "\n",
    "- Nella matrice estratta da `CountVectorizer` abbiamo il numero di occorrenze dei termini nei documenti\n",
    "- Esistono però metodi più avanzati per pesare la rilevanza di ciascun termine in un documento\n",
    "- È di uso comune il _tf.idf_ (_term frequency-inverse document frequency_), pari al prodotto di due fattori (di cui esistono diverse formulazioni)\n",
    "  - il _tf_ indica l'**importanza locale** di un termine in un documento ed è pari al numero di occorrenze (o al suo logaritmo)\n",
    "  - l'_idf_ indica l'**importanza globale** di un termine, tanto più alta quanto più il termine è poco comune nell'insieme complessivo dei documenti\n",
    "  - comunemente, una volta calcolati tutti i pesi, **ciascun vettore è normalizzato** in modo da avere norma euclidea pari a 1, per appianare differenze di pesi tra documenti più o meno lunghi\n",
    "- `TfidfVectorizer` è un filtro utilizzabile come alternativa a `CountVectorizer` per estrarre matrici documenti-termini basate sul tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Calcoliamo ad esempio la matrice dei tf.idf per l'insieme di 4 frasi usate sopra..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the sky is blue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.603137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky is blue and sky is beautiful</th>\n",
       "      <td>0.440516</td>\n",
       "      <td>0.347308</td>\n",
       "      <td>0.229880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the beautiful sky is so blue</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432026</td>\n",
       "      <td>0.285953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349762</td>\n",
       "      <td>0.54797</td>\n",
       "      <td>0.432026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i love blue cheese</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346182</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  beautiful      blue    cheese  \\\n",
       "the sky is blue                   0.000000   0.000000  0.399210  0.000000   \n",
       "sky is blue and sky is beautiful  0.440516   0.347308  0.229880  0.000000   \n",
       "the beautiful sky is so blue      0.000000   0.432026  0.285953  0.000000   \n",
       "i love blue cheese                0.000000   0.000000  0.346182  0.663385   \n",
       "\n",
       "                                        is      love       sky       so  \\\n",
       "the sky is blue                   0.488291  0.000000  0.488291  0.00000   \n",
       "sky is blue and sky is beautiful  0.562351  0.000000  0.562351  0.00000   \n",
       "the beautiful sky is so blue      0.349762  0.000000  0.349762  0.54797   \n",
       "i love blue cheese                0.000000  0.663385  0.000000  0.00000   \n",
       "\n",
       "                                       the  \n",
       "the sky is blue                   0.603137  \n",
       "sky is blue and sky is beautiful  0.000000  \n",
       "the beautiful sky is so blue      0.432026  \n",
       "i love blue cheese                0.000000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "dtm = vect.fit_transform(docs)\n",
    "pd.DataFrame(dtm.toarray(), index=docs, columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ad es. nell'ultimo documento \"cheese\" ha un peso maggiore di \"blue\"\n",
    "  - entrambe compaiono una volta nel documento (tf=1)\n",
    "  - ma \"cheese\" è una parola meno comune in altri documenti e quindi più discriminante (idf maggiore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per testare l'uso del tf.idf nel classificatore di recensioni, sostituiamo il `CountVectorizer` con il `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Addestriamo il modello sul training set e valutiamo l'accuratezza sul validation set come sopra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256666666666667"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"])\n",
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scomposizione in termini\n",
    "\n",
    "- I vectorizer (`Count` e `Tfidf`) usano un algoritmo semplice per scomporre le parole nei documenti\n",
    "- Possiamo anche passare come parametro `tokenizer` una funzione alternativa che prenda in input una stringa e restituisca la lista di \"token\" (parole) in essa\n",
    "- Specifichiamo ad esempio di usare il metodo `split` delle stringhe, che suddivide in base agli spazi ignorando i segni di punteggiatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.07 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<method 'split' of 'str' objects>)),\n",
       "                ('classifier', LogisticRegression(C=10, solver='saga'))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(tokenizer=str.split)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il modello ottenuto può essere leggermente meno preciso per questo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 462 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8093333333333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Usando la funzione `word_tokenize` di NLTK il tempo di preprocessamento aumenta ma il modello può essere più accurato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function word_tokenize at 0x00000222D6732F70>)),\n",
       "                ('classifier', LogisticRegression(C=10, solver='saga'))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(tokenizer=nltk.word_tokenize)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.822"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Riduzione della dimensionalità\n",
    "\n",
    "- Il numero di dimensioni generate considerando tutti i termini distinti presenti in tutti i documenti, come anche visto sopra, è molto alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72509"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All'aumentare del numero di documenti, una tale quantità di dimensioni può comportare tempi di calcolo e uso di memoria eccessivi\n",
    "- Esistono però modi per ridurre il numero di dimensioni con effetti minimi sull'accuratezza del modello\n",
    "- Impostando il parametro `min_df` di `TfidfVectorizer` (o `CountVectorizer`), limitiamo le parole nel dizionario dello spazio vettoriale a quelle presenti in almeno _N_ documenti di training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Creiamo ad esempio un modello limitato alle parole che appaiono in almeno 3 documenti di training distinti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il numero di feature è molto inferiore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21063"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...ma l'accuratezza è quasi identica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8226666666666667"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rimozione stopword\n",
    "\n",
    "- Sono dette _stopword_ quelle parole che prese da sole non danno alcuna informazione sulla semantica del testo\n",
    "  - sono stopword ad es. articoli (\"le\"), preposizioni (\"per\"), congiunzioni (\"ma\"), ...\n",
    "- Nella rappresentazione BOW di documenti è comune rimuovere a priori le stopword, in quanto non informative del contenuto\n",
    "- Esistono diverse liste di stopword, NLTK ne integra alcune per diverse lingue\n",
    "- Reperiamo la lista delle stopword inglesi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Federico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La lista include ad esempio le parole..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i    me    my    myself    we    our    ours    ourselves    you    you're\n"
     ]
    }
   ],
   "source": [
    "print(\"    \".join( stoplist[:10] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Nei vectorizer, possiamo configurare una lista di parole da escludere con un parametro `stop_words`\n",
    "  - di default il valore è `None`: non si usa alcuna stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, stop_words=stoplist)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rimuovendo le stopword, così come accade impostando `min_df`, otteniamo generalmente una riduzione delle feature con variazioni contenute di accuratezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20922"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8096666666666666"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 5: Grid search per riduzione dimensionalità\n",
    "\n",
    "- **(5a)** Eseguire una grid search sul training set con cross validation a 3 fold stratificati per ottimizzare i parametri di un modello basato su tf.idf e regressione logistica simile a quelli sopra con\n",
    "  - numero minimo di documenti in cui deve apparire una parola pari a 3, 5 o 10\n",
    "  - nessuna rimozione delle stopword o rimozione delle stopword nella lista `stoplist`\n",
    "- **(5b)** Valutare l'accuratezza sul validation set del modello con la configurazione migliore individuata dalla grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(3, shuffle=True, random_state=42)\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "grid = {\n",
    "    \"vectorizer__min_df\" : [3,5,10],\n",
    "    \"vectorizer__stop_words\" : [None, stoplist]\n",
    "}\n",
    "gs = GridSearchCV(model, grid, cv=skf)\n",
    "gs.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer__min_df': 3, 'vectorizer__stop_words': None}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vectorizer__min_df</th>\n",
       "      <th>param_vectorizer__stop_words</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.118699</td>\n",
       "      <td>0.051577</td>\n",
       "      <td>0.356968</td>\n",
       "      <td>0.039510</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vectorizer__min_df': 3, 'vectorizer__stop_words': None}</td>\n",
       "      <td>0.806769</td>\n",
       "      <td>0.812688</td>\n",
       "      <td>0.817402</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.090506</td>\n",
       "      <td>0.048542</td>\n",
       "      <td>0.368821</td>\n",
       "      <td>0.036644</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vectorizer__min_df': 10, 'vectorizer__stop_words': None}</td>\n",
       "      <td>0.811911</td>\n",
       "      <td>0.808401</td>\n",
       "      <td>0.815688</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.123019</td>\n",
       "      <td>0.089627</td>\n",
       "      <td>0.352060</td>\n",
       "      <td>0.010166</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vectorizer__min_df': 5, 'vectorizer__stop_words': None}</td>\n",
       "      <td>0.806769</td>\n",
       "      <td>0.811402</td>\n",
       "      <td>0.815259</td>\n",
       "      <td>0.811143</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.972911</td>\n",
       "      <td>0.024179</td>\n",
       "      <td>0.318255</td>\n",
       "      <td>0.016771</td>\n",
       "      <td>3</td>\n",
       "      <td>[i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, y...</td>\n",
       "      <td>{'vectorizer__min_df': 3, 'vectorizer__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ou...</td>\n",
       "      <td>0.804199</td>\n",
       "      <td>0.807544</td>\n",
       "      <td>0.807544</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.947410</td>\n",
       "      <td>0.039372</td>\n",
       "      <td>0.305926</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>10</td>\n",
       "      <td>[i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, y...</td>\n",
       "      <td>{'vectorizer__min_df': 10, 'vectorizer__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'o...</td>\n",
       "      <td>0.802485</td>\n",
       "      <td>0.804972</td>\n",
       "      <td>0.809258</td>\n",
       "      <td>0.805572</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.118699      0.051577         0.356968        0.039510   \n",
       "4       1.090506      0.048542         0.368821        0.036644   \n",
       "2       1.123019      0.089627         0.352060        0.010166   \n",
       "1       0.972911      0.024179         0.318255        0.016771   \n",
       "5       0.947410      0.039372         0.305926        0.005454   \n",
       "\n",
       "  param_vectorizer__min_df  \\\n",
       "0                        3   \n",
       "4                       10   \n",
       "2                        5   \n",
       "1                        3   \n",
       "5                       10   \n",
       "\n",
       "                                                                          param_vectorizer__stop_words  \\\n",
       "0                                                                                                 None   \n",
       "4                                                                                                 None   \n",
       "2                                                                                                 None   \n",
       "1  [i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, y...   \n",
       "5  [i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, y...   \n",
       "\n",
       "                                                                                                params  \\\n",
       "0                                            {'vectorizer__min_df': 3, 'vectorizer__stop_words': None}   \n",
       "4                                           {'vectorizer__min_df': 10, 'vectorizer__stop_words': None}   \n",
       "2                                            {'vectorizer__min_df': 5, 'vectorizer__stop_words': None}   \n",
       "1  {'vectorizer__min_df': 3, 'vectorizer__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ou...   \n",
       "5  {'vectorizer__min_df': 10, 'vectorizer__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'o...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.806769           0.812688           0.817402         0.812287   \n",
       "4           0.811911           0.808401           0.815688         0.812000   \n",
       "2           0.806769           0.811402           0.815259         0.811143   \n",
       "1           0.804199           0.807544           0.807544         0.806429   \n",
       "5           0.802485           0.804972           0.809258         0.805572   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.004350                1  \n",
       "4        0.002975                2  \n",
       "2        0.003471                3  \n",
       "1        0.001577                4  \n",
       "5        0.002798                5  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs.cv_results_).sort_values(\"rank_test_score\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## n-gram\n",
    "\n",
    "- Un _n-gram_ è una **sequenza di _n_ parole consecutive** presenti in un testo\n",
    "  - nei casi più comuni con n pari a 2 o 3 si parla rispettivamente di _bigram_ o _trigram_\n",
    "  - ad es., nella frase \"vado a New York\", i bigram sono \"vado a\", \"a New\" e \"New York\"\n",
    "- Così come le parole singole, anche gli n-gram possono essere usati come **feature per rappresentare i documenti**\n",
    "  - alcuni n-gram possono essere significativi, rappresentando un termine composto da più parole (es. \"New York\")\n",
    "  - ne rimangono però molti senza un significato specifico (es. \"a New\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Impostando in un vectorizer l'attributo `ngram_range` ad una **tupla `(a, b)`**, sono usate come feature le possibili **sequenze contenenti dalle a alle b parole**\n",
    "  - l'impostazione di default è `(1, 1)`, per cui sono selezionate solo parole singole\n",
    "  - impostando invece ad es. `(1, 2)` si selezionano sia le parole singole che i bigram\n",
    "  - introducendo gli n-gram **il numero di termini distinti aumenta di molto**: è importante ridurli specificando un `min_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, ngram_range=(1, 2))),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100711"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8233333333333334"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- I termini estratti includono parole singole e bigram significativi e non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zone and',\n",
       " 'zone feel',\n",
       " 'zone the',\n",
       " 'zones',\n",
       " 'zoo',\n",
       " 'zooey',\n",
       " 'zooey deschanel',\n",
       " 'zoolander',\n",
       " 'zoom',\n",
       " 'zooming']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_steps[\"vectorizer\"].get_feature_names()[-15:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## POS Tagging\n",
    "\n",
    "- Ad ogni parola in una frase è associata una **classe grammaticale**, detta _Part of Speech_ (POS)\n",
    "  - ad alto livello le POS sono **_nome_, _verbo_, _aggettivo_, ecc.**\n",
    "  - spesso si effettuano ulteriori distinzioni, ad es. nomi singolari e plurali\n",
    "  - le POS possono includere anche i segni di punteggiatura\n",
    "- Il _POS tagging_ etichetta ogni token di una sequenza (ottenuta dalla segmentazione) con il suo POS\n",
    "  - è un processo non banale, in quanto una stessa parola può avere diverse POS a seconda del contesto (es. \"letto\" può essere nome o verbo)\n",
    "- La funzione `pos_tag` prende una sequenza di token e restituisce una lista di tuple `(token, tag)`\n",
    "  - i possibili tag sono elencati quì: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Federico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scarico dei dati necessari per il POS tagging\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This    is    n't    an    example    ,    or    is    it    ?\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"This isn't an example, or is it?\")\n",
    "print(\"    \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " (\"n't\", 'RB'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_with_pos = nltk.pos_tag(tokens)\n",
    "tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (DT)    (VBZ)   (RB)    (DT)    (NN)     (,)    (CC)    (VBZ)   (PRP)    (.)  \n",
      " This     is      n't     an    example    ,      or      is      it       ?   \n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(\"{:^7}\".format(f\"({pos})\") for _, pos  in tokens_with_pos))\n",
    "print(\" \".join(\"{:^7}\".format(word)       for word, _ in tokens_with_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 6: Bag of words con POS tagging\n",
    "\n",
    "- **(6a)** Implementare la funzione `tokenize_with_pos` in modo che, dato un testo, restituisca la lista di tuple `(parola, POS)` come quella sopra\n",
    "- **(6b)** Addestrare sul training set un modello di classificazione con `TfidfVectorizer` simile a quelli sopra (senza usare gli n-gram), utilizzando la funzione `tokenize_with_pos` per la scomposizione dei documenti in parole\n",
    "  - le tuple `(parola, POS)` sono accettate come nomi delle feature\n",
    "- **(6c)** Estrarre il numero di feature generate dal vectorizer\n",
    "- **(6d)** Valutare sul validation set l'accuratezza del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_pos(text):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero di features = 26838\n",
      "R^2 = 0.815\n"
     ]
    }
   ],
   "source": [
    "# molto lento\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3,tokenizer=tokenize_with_pos)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"])\n",
    "nFeatures = len(model.named_steps[\"vectorizer\"].get_feature_names())\n",
    "score = model.score(reviews_val[\"text\"], reviews_val[\"label\"])\n",
    "print(f\"numero di features = {nFeatures}\")\n",
    "print(f\"R^2 = {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatizzazione\n",
    "\n",
    "- Estraendo dai documenti tutte le parole nella forma in cui sono scritte, otteniamo spesso molteplici forme di uno stesso _lemma_ (parola di un vocabolario)\n",
    "  - es. le parole \"estraggo\", \"estraendo\", \"estratto\", ... sono tutte coniugazioni diverse del verbo \"estrarre\"\n",
    "- La _lemmatizzazione_ è il processo che converte ciascuna parola di un testo nel suo lemma\n",
    "  - nomi al singolare, verbi all'infinito, ...\n",
    "- In questo modo **raggruppiamo gruppi di termini simili**, riducendo la dimensionalità dello spazio senza perdita di informazione rilevante\n",
    "- Per eseguire la lemmatizzazione in NLTK creiamo un oggetto `WordNetLemmatizer`\n",
    "  - dobbiamo scaricare la base di conoscenza WordNet, che include le informazioni necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Federico\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "# in versioni più recenti:\n",
    "# nltk.download(\"omw-1.4\")\n",
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per lemmatizzare una parola si usa il metodo `lemmatize`, passandone anche il POS\n",
    "  - i POS possibili sono: n=nome, v=verbo, a=aggettivo, r=avverbio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esempio: nome con plurale regolare\n",
    "wnl.lemmatize(\"words\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esempio: nome con plurale irregolare\n",
    "wnl.lemmatize(\"mice\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esempio: verbo con forma passata irregolare\n",
    "wnl.lemmatize(\"went\", \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Creiamo una funzione che segmenti le parole di un testo eseguendo la lemmatizzazione dove possibile\n",
    "  - creiamo un dizionario con le corrispondenze tra le prime lettere dei POS in Penn Treebank visti sopra e questi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_to_wn = {\"N\": \"n\", \"V\": \"v\", \"J\": \"a\", \"R\": \"r\"}\n",
    "def tokenize_with_lemmatization(text):\n",
    "    return [(wnl.lemmatize(token, penn_to_wn[tag[0]]) if tag[0] in penn_to_wn else token)\n",
    "            for token, tag in nltk.pos_tag(nltk.tokenize.word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'have', 'show', 'many', 'example', '!']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_lemmatization(\"We have shown many examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La funzione ha estratto le parole, ma portando il verbo \"show\" e il nome \"example\" alle forme base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Usiamo la funzione come `tokenizer` per la creazione dello spazio vettoriale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# molto lento\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3, tokenizer=tokenize_with_lemmatization)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anche in questo modo otteniamo un numero di feature inferiore, tuttavia il tempo impiegato per il processamento del testo è elevato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18153"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.named_steps[\"vectorizer\"].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# molto lento\n",
    "model.score(reviews_val[\"text\"], reviews_val[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming\n",
    "- Un algoritmo di _stemming_ estrae da una parola la sua **radice morfologica**\n",
    "- Al contrario di un lemma, la radice di una parola può non essere di senso compiuto\n",
    "- Termini diversi (anche come lemma) possono avere la stessa radice\n",
    "- Ciònonostante, lo stemming è spesso usato come **alternativa alla lemmatizzazione**, in quanto\n",
    "  - lemmi con la stessa radice sono spesso correlati (es. nome \"pesce\" e verbo \"pescare\")\n",
    "  - lo stemming non richiede il POS tagging ed è più efficiente\n",
    "- NLTK integra diversi algoritmi di stemming, tra cui ad es. `PorterStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lemmat', 'lemmat', 'lemmat')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "ps.stem(\"lemmatization\"), ps.stem(\"lemmatizer\"), ps.stem(\"lemmatize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 7: Bag of words con stemming\n",
    "\n",
    "- **(7a)** Implementare la funzione `tokenize_with_stemming` in modo che, dato un testo, restituisca la lista di parole in esso con stemming applicato\n",
    "- **(7b)** Addestrare sul training set un modello di classificazione con `TfidfVectorizer` simile a quelli sopra (senza usare gli n-gram), utilizzando la funzione `tokenize_with_stemming` per la scomposizione dei documenti in parole\n",
    "- **(7c)** Estrarre il numero di feature generate dal vectorizer\n",
    "- **(7d)** Valutare sul validation set l'accuratezza del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_stemming(text):\n",
    "    return [ps.stem(token) for token in nltk.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero di features = 15927\n",
      "R^2 = 0.8233333333333334\n"
     ]
    }
   ],
   "source": [
    "# molto lento\n",
    "model = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=3,tokenizer=tokenize_with_stemming)),\n",
    "    (\"classifier\", LogisticRegression(solver=\"saga\", C=10))\n",
    "])\n",
    "model.fit(reviews_train[\"text\"], reviews_train[\"label\"])\n",
    "nFeatures = len(model.named_steps[\"vectorizer\"].get_feature_names())\n",
    "score = model.score(reviews_val[\"text\"], reviews_val[\"label\"])\n",
    "print(f\"numero di features = {nFeatures}\")\n",
    "print(f\"R^2 = {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentiment Analysis in NLTK\n",
    "\n",
    "- La stima dell'orientamento positivo o negativo di opinioni scritte è un problema molto comune\n",
    "- Per questo esistono modelli preaddestrati, tra cui [_VADER_](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8109) (_Valence Aware Dictionary and sEntiment Reasoner_)\n",
    "- NLTK permette di utilizzare VADER per valutare l'orientamento di opinioni, senza bisogno di addestrare modelli\n",
    "- Per usare VADER, scarichiamo i dati necessari e creiamo un oggetto `SentimentIntensityAnalyzer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Federico\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- VADER si basa su un insieme di regole e su un dizionario (_lexicon_), che associa ad ogni parola un punteggio che ne denota la valenza positiva o negativa\n",
    "- Oltre a parole della lingua inglese, il dizionario prevede anche termini abbreviati, slang e emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\"excellent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\"sux\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\"n00b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.lexicon[\":)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Per stimare la polarità di una frase, usiamo il metodo `polarity_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores(\"What an awesome movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.464, 'neu': 0.536, 'pos': 0.0, 'compound': -0.3804}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores(\"It was really boring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores(\"This movie was shot in Italy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I valori `pos`, `neg` e `neu` valutano quanto la frase sia positiva, negativa o neutra\n",
    "- `compound` è un'aggregazione dei tre punteggi e riassume la polarità della frase\n",
    "- Se abbiamo una recensione completa, possiamo usare la funzione `sent_tokenize` di NLTK per scomporla in singole frasi valutabili con VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nice movie!', 'Actors were really good.', 'Excellent!']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(\"Nice movie! Actors were really good. Excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Esercizio 8: Utilizzo di VADER come modello di classificazione\n",
    "\n",
    "- **(8a)** Implementare la funzione `label_review` in modo che\n",
    "  - scomponga il testo di una recensione data in frasi\n",
    "  - estragga per ogni frase il punteggio _compound_ dato da VADER\n",
    "  - restituisca `\"pos\"` se la somma dei punteggi è positiva, `\"neg\"` altrimenti\n",
    "- **(8b)** Usare la funzione per estrarre una sequenza (lista, array o serie) delle classi previste da VADER per le recensioni del validation set\n",
    "- **(8c)** Usare la funzione `accuracy_score(classi_reali, classi_predette)` di scikit-learn per calcolare l'accuratezza delle predizioni\n",
    "  - l'accuratezza può risultare inferiore rispetto ai modelli sopra, in quanto questi ultimi sono addestrati specificamente sulle nostre recensioni mentre VADER è più orientato ai tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_review(review):\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    # scores = [vader.polarity_scores(s) for s in sentences]\n",
    "    scores = list(map(vader.polarity_scores, sentences))\n",
    "    compound = [s[\"compound\"] for s in scores]\n",
    "    total = sum(compound)\n",
    "    return \"pos\" if total >= 0 else \"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_review('fuck movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " ...]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_preds = [label_review(review) for review in reviews_val[\"text\"]]\n",
    "vader_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
